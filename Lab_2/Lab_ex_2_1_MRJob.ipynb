{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Lab_ex_2_1_MRJob.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
<<<<<<< Updated upstream
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_-d86u_XkJ"
      },
      "source": [
        "# Ex 2.1 Hadoop MapReduce with Python\n",
        "There are two prominent *Python* APIs for interfacing *Hadoop MapReduce* clusters:\n",
        "\n",
        "## *Snakebite* for *HDFS* access\n",
        "The [Snakebite Lib](https://github.com/spotify/snakebite) allows easy access to *HDFS* file systems:  \n",
        "```\n",
        ">>> from snakebite.client import Client\n",
        ">>> client = Client(\"localhost\", 8020, use_trash=False)\n",
        ">>> for x in client.ls(['/']):\n",
        "...     print x\n",
        "```\n",
        "\n",
        "See [documentation](https://snakebite.readthedocs.io/en/latest/) for details.\n",
        "\n",
        "\n",
        "## *MRJOB* for *MapReduce* job execution\n",
        "The ``mrjob`` lib -> [see docu](https://mrjob.readthedocs.io/en/latest/index.html) is a power full *MapReduce* client for *Python*. Some of the key features are:\n",
        "\n",
        "* local emulation (single and multi-core) a *Hadoop* cluster for development and debugging\n",
        "* simple access, authentication and file transfer to *Hadoop* clusters\n",
        "* powerful API for common cloud services, such as AWS or Azure   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdzU2hvc_XkN"
      },
      "source": [
        "### Preparing our environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftS_MV--_XkO",
        "outputId": "79b0e28c-30cb-46c4-896a-06c472d696cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#install mrjob lib and boto3 for AWS S3 access\n",
        "#!conda install -c conda-forge -y mrjob boto3\n",
        "\n",
        "!pip install mrjob boto3"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mrjob in /usr/local/lib/python3.7/dist-packages (0.7.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.17.87)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from mrjob) (3.13)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.87 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.20.87)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.4.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.87->boto3) (1.26.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.87->boto3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.87->boto3) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K48-ZIC_XkT"
      },
      "source": [
        "## A *MRJOB* Example: WordCount (again)\n",
        "Since *Hadoop* works only on file in- and outputs, we do not have usual function based API. We need to pass our code (implementation of *Map* and *Reduce*) as executable *Python* scripts:\n",
        "\n",
        "* use *Jupyter's* ``%%file`` magic command to write the cell to file\n",
        "* create a executable script with ``__main__`` method\n",
        "* inherit from the ``MRJob`` class\n",
        "* implement ``mapper()`` and ``reducer()`` methods\n",
        "* call ``run()`` at start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-gg4mlM_XkW",
        "outputId": "8f428996-0339-4464-fc23-b5637f42a3c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file wordcount.py \n",
        "#this will save this cell as file\n",
        "\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class MRWordCount(MRJob):\n",
        "    def mapper(self, _, line):\n",
        "        for word in line.split():\n",
        "            yield(word, 1)\n",
        " \n",
        "    def reducer(self, word, counts):\n",
        "        yield(word, sum(counts))\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    MRWordCount.run()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting wordcount.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epIxdkI-_XkX"
      },
      "source": [
        "### execute script from cmd\n",
        "* ``-r local`` causes local multi-core emulation a *Hadoop* cluster.\n",
        "* Input files are cmd arguments\n",
        "* define ouput-file (see docs) or use streams: `` > out.txt``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_nUc0uje_XkX",
        "outputId": "a1e99bb1-a946-432a-e5a6-7c3c100561a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! python wordcount.py -r local *.rst > out.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for local runner\n",
            "Creating temp directory /tmp/wordcount.root.20210604.124659.188971\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/wordcount.root.20210604.124659.188971/output\n",
            "Streaming final output from /tmp/wordcount.root.20210604.124659.188971/output...\n",
            "Removing temp directory /tmp/wordcount.root.20210604.124659.188971...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kK20LbB_XkY"
      },
      "source": [
        " -> results in **out.txt** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZGeghhb_XkZ"
      },
      "source": [
        "## Execution on AWS EMR\n",
        "AWS EMR is a clound formation service which allows you to create *Hadoop*, *Spark* and other data analytics clusters with a few clicks.\n",
        "\n",
        "**NOTE**: we are not endorsing AWS specifically, other cloud service providers have similar offers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tuLu7xo_Xka"
      },
      "source": [
        "### Connect to existing cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnd6heQe_Xkb",
        "outputId": "1b828e9c-9dd9-4c0a-f5fa-ba6a976b183e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file mrjob_cluster.conf\n",
        "runners:\n",
        "  emr:\n",
        "    aws_access_key_id: AKIA4KIF2TSEWQK7VOF4\n",
        "    aws_secret_access_key: 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\n",
        "    region: eu-west-1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing mrjob_cluster.conf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p_AC9-G_Xkc"
      },
      "source": [
        "We need the **ID** of the cluster we want to connect to - here pre-set to our Cluster today"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDLbm7kz_Xkd"
      },
      "source": [
        "! python wordcount.py -r emr --cluster-id=j-L1BO0NYZIYY0 text1.rst text2.rst -c mrjob_cluster.conf  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUc3ArB9_Xke"
      },
      "source": [
        "## Exercise\n",
        "Use  *mrjob*  to  compute  employee  **top  annual  salaries** and  **gross pay** in the *CSV* table ``Baltimore_City_employee_Salaries_FY2014.csv``.\n",
        "\n",
        "* use  ``import csv`` to read the data -> [API docs](https://docs.python.org/3/library/csv.html)\n",
        "* use ``yield`` to return *producers* from *map* and *reduce* functions\n",
        "* return top entries in both categories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM2C08V5_Xkf"
      },
      "source": [
        "import csv\n",
        "with open('Baltimore_City_Employee_Salaries_FY2014.csv', newline='') as csvfile:\n",
        "  reader = csv.DictReader(csvfile)\n",
        "  for row in reader:\n",
        "    print(row['AnnualSalary'], row['GrossPay'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W9-IGziF4Fk"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL28-NNGDxMu",
        "outputId": "7b7bff07-d99c-404c-cdfd-9df0874d20e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file MRComputeSalaries.py \n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "cols = 'Name,JobTitle,AgencyID,Agency,HireDate,AnnualSalary,GrossPay'.split(',')\n",
        "\n",
        "class salarymax(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        # Convert each line into a dictionary\n",
        "        row = dict(zip(cols, [ a.strip() for a in next(csv.reader([line]))]))\n",
        "\n",
        "        # Yield the salary\n",
        "        try:\n",
        "          yield 'salary', (float(row['AnnualSalary']), line)\n",
        "        except ValueError:\n",
        "          self.increment_counter('warn', 'missing salary', 1)\n",
        "        \n",
        "        # Yield the gross pay\n",
        "        try:\n",
        "            yield 'gross', (float(row['GrossPay']), line)\n",
        "        except ValueError:\n",
        "            self.increment_counter('warn', 'missing gross', 1)\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        topten = []\n",
        "\n",
        "        # For 'salary' and 'gross' compute the top 10\n",
        "        for p in values:\n",
        "            topten.append(p)\n",
        "            topten.sort()\n",
        "            topten = topten[-10:]\n",
        "\n",
        "        for p in topten:\n",
        "            yield key, p\n",
        "\n",
        "    combiner = reducer\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    salarymax.run()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting MRComputeSalaries.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neAUytFONEyH",
        "outputId": "906e4889-a54e-4e12-d998-e85906aac22f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python MRComputeSalaries.py -r local Baltimore_City_Employee_Salaries_FY2014.csv > top_salaries.txt\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for local runner\n",
            "Creating temp directory /tmp/MRComputeSalaries.root.20210604.141925.498227\n",
            "Running step 1 of 1...\n",
            "\n",
            "Counters: 2\n",
            "\twarn\n",
            "\t\tmissing gross=3224\n",
            "\t\tmissing salary=1\n",
            "\n",
            "job output is in /tmp/MRComputeSalaries.root.20210604.141925.498227/output\n",
            "Streaming final output from /tmp/MRComputeSalaries.root.20210604.141925.498227/output...\n",
            "Removing temp directory /tmp/MRComputeSalaries.root.20210604.141925.498227...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHF14eBZ-tX",
        "outputId": "0e4294a6-db0e-4465-a214-47be6022df20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! python  MRComputeSalaries.py -r emr --cluster-id=j-L1BO0NYZIYY0 Baltimore_City_Employee_Salaries_FY2014.csv -c mrjob_cluster.conf  > top_salaries2.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using s3://mrjob-42e7145df80ebe94/tmp/ as our temp dir on S3\n",
            "Creating temp directory /tmp/MRComputeSalaries.root.20210604.142351.359505\n",
            "uploading working dir files to s3://mrjob-42e7145df80ebe94/tmp/MRComputeSalaries.root.20210604.142351.359505/files/wd...\n",
            "Copying other local files to s3://mrjob-42e7145df80ebe94/tmp/MRComputeSalaries.root.20210604.142351.359505/files/\n",
            "Adding our job to existing cluster j-L1BO0NYZIYY0\n",
            "  master node is ec2-52-212-16-94.eu-west-1.compute.amazonaws.com\n",
            "Waiting for Step 1 of 1 (s-39HK2XAPQKEA8) to complete...\n",
            "  PENDING (cluster is RUNNING: Running step)\n",
            "  PENDING (cluster is RUNNING: Running step)\n",
            "  RUNNING for 0:00:59\n",
            "  COMPLETED\n",
            "Attempting to fetch counters from logs...\n",
            "Waiting 10 minutes for logs to transfer to S3... (ctrl-c to skip)\n",
            "\n",
            "To fetch logs immediately next time, set up SSH. See:\n",
            "https://pythonhosted.org/mrjob/guides/emr-quickstart.html#configuring-ssh-credentials\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
=======
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install mrjob lib and boto3 for AWS S3 access\n",
    "!conda install -c conda-forge -y mrjob boto3\n",
    "\n",
    "#!pip install mrjob boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A *MRJOB* Example: WordCount (again)\n",
    "Since *Hadoop* works only on file in- and outputs, we do not have usual function based API. We need to pass our code (implementation of *Map* and *Reduce*) as executable *Python* scripts:\n",
    "\n",
    "* use *Jupyter's* ``%%file`` magic command to write the cell to file\n",
    "* create a executable script with ``__main__`` method\n",
    "* inherit from the ``MRJob`` class\n",
    "* implement ``mapper()`` and ``reducer()`` methods\n",
    "* call ``run()`` at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file wordcount.py \n",
    "#this will save this cell as file\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield(word, 1)\n",
    " \n",
    "    def reducer(self, word, counts):\n",
    "        yield(word, sum(counts))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute script from cmd\n",
    "* ``-r local`` causes local multi-core emulation a *Hadoop* cluster.\n",
    "* Input files are cmd arguments\n",
    "* define ouput-file (see docs) or use streams: `` > out.txt``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python wordcount.py -r local *.rst > out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -> results in **out.txt** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution on AWS EMR\n",
    "AWS EMR is a clound formation service which allows you to create *Hadoop*, *Spark* and other data analytics clusters with a few clicks.\n",
    "\n",
    "**NOTE**: we are not endorsing AWS specifically, other cloud service providers have similar offers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to existing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file mrjob_cluster.conf\n",
    "runners:\n",
    "  emr:\n",
    "    aws_access_key_id: YOUR_KEY_ID\n",
    "    aws_secret_access_key: YOUR_KEY_SECRET\n",
    "    region: eu-west-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the **ID** of the cluster we want to connect to - here pre-set to our Cluster today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python wordcount.py -r emr --cluster-id=j-L1BO0NYZIYY0 text1.rst text2.rst -c mrjob_cluster.conf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Use  *mrjob*  to  compute  employee  **top  annual  salaries** and  **gross pay** in the *CSV* table ``Baltimore_City_employee_Salaries_FY2014.csv``.\n",
    "\n",
    "* use  ``import csv`` to read the data -> [API docs](https://docs.python.org/3/library/csv.html)\n",
    "* use ``yield`` to return *producers* from *map* and *reduce* functions\n",
    "* return top entries in both categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('Baltimore_City_Employee_Salaries_FY2014.csv', newline='') as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "  for row in reader:\n",
    "    print(row['AnnualSalary'], row['GrossPay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file MRComputeSalaries.py \n",
    "#this will save this cell as file\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRComputeSalaries(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield(word, 1)\n",
    " \n",
    "    def reducer(self, word, counts):\n",
    "        yield(word, sum(counts))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRComputeSalariest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> Stashed changes
